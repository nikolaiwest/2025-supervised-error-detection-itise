from typing import Any, Dict, List

import numpy as np

from .fold_result import FoldResult


class ModelResult:
    """Results from evaluating a model across all cross-validation folds."""

    def __init__(self, model_name: str, dataset_name: str):
        """Initialize model result to hold multiple fold results."""
        # Set names for model and dataset
        self.model_name = model_name
        self.dataset_name = dataset_name

        # Initialize an empty list to hold all fold results
        self.fold_results: List[FoldResult] = []

        # Initialize empty metrics dictionaries (will be computed when folds are added)
        self.mean_metrics = {}
        self.std_metrics = {}
        self.min_metrics = {}
        self.max_metrics = {}
        self.confusion_matrix = None

    def add_result(self, fold_result: FoldResult) -> None:
        """Add a fold result and recompute aggregated metrics."""
        if not isinstance(fold_result, FoldResult):
            raise TypeError("fold_result must be a FoldResult instance")

        self.fold_results.append(fold_result)

        # Recompute aggregated metrics after each fold addition
        # This enables real-time updates in MLflow
        self._compute_aggregated_metrics()
        self._compute_aggregated_confusion_matrix()

    def get_metrics(self) -> dict[str, Any]:
        """
        Get comprehensive metrics dictionary based on completed folds.

        Returns different information based on number of completed folds:
        - 0 folds: Zero metrics as placeholders
        - 1 fold: Single fold results
        - Multiple folds: Averaged results with statistics
        """
        n_folds = len(self.fold_results)

        if n_folds == 0:
            # Return placeholder metrics when no folds completed
            return {
                "accuracy": 0.0,
                "precision": 0.0,
                "recall": 0.0,
                "f1_score": 0.0,
                "n_folds": 0,
                "status": "no_folds_completed",
            }

        elif n_folds == 1:
            # Return single fold results
            single_fold = self.fold_results[0]
            metrics = single_fold.metrics.copy()
            metrics.update(
                {
                    "n_folds": 1,
                    "status": "single_fold",
                    "training_time": single_fold.training_time,
                    "prediction_time": single_fold.prediction_time,
                }
            )
            return metrics

        else:
            # Return aggregated results across all folds
            result_metrics = {}

            # Add mean metrics (primary values)
            for metric_name, value in self.mean_metrics.items():
                result_metrics[metric_name] = value

            # Add statistical information
            for metric_name in self.mean_metrics.keys():
                result_metrics[f"{metric_name}_std"] = self.std_metrics.get(
                    metric_name, 0.0
                )
                result_metrics[f"{metric_name}_min"] = self.min_metrics.get(
                    metric_name, 0.0
                )
                result_metrics[f"{metric_name}_max"] = self.max_metrics.get(
                    metric_name, 0.0
                )

                # Add coefficient of variation for stability assessment
                cv = self.std_metrics.get(metric_name, 0.0) / max(
                    self.mean_metrics.get(metric_name, 1.0), 1e-10
                )
                result_metrics[f"{metric_name}_cv"] = cv

            # Add timing statistics
            if self.fold_results:
                train_times = [fold.training_time for fold in self.fold_results]
                pred_times = [fold.prediction_time for fold in self.fold_results]

                result_metrics.update(
                    {
                        "avg_training_time": np.mean(train_times),
                        "total_training_time": np.sum(train_times),
                        "avg_prediction_time": np.mean(pred_times),
                        "total_prediction_time": np.sum(pred_times),
                    }
                )

            # Add metadata
            result_metrics.update({"n_folds": n_folds, "status": "aggregated"})

            return result_metrics

    def _compute_aggregated_metrics(self):
        """Compute mean, std, min, max across all folds."""
        if not self.fold_results:
            self.mean_metrics = {}
            self.std_metrics = {}
            self.min_metrics = {}
            self.max_metrics = {}
            return

        # Get all metric names from first fold
        metric_names = self.fold_results[0].metrics.keys()

        self.mean_metrics = {}
        self.std_metrics = {}
        self.min_metrics = {}
        self.max_metrics = {}

        for metric_name in metric_names:
            values = [fold.metrics.get(metric_name, 0) for fold in self.fold_results]
            self.mean_metrics[metric_name] = np.mean(values)
            self.std_metrics[metric_name] = np.std(values)
            self.min_metrics[metric_name] = np.min(values)
            self.max_metrics[metric_name] = np.max(values)

    def _compute_aggregated_confusion_matrix(self):
        """Compute confusion matrix aggregated across all folds."""
        if not self.fold_results:
            self.confusion_matrix = None
            return

        # Concatenate all predictions and true labels
        all_y_true = np.concatenate([fold.y_true for fold in self.fold_results])
        all_y_pred = np.concatenate([fold.y_pred for fold in self.fold_results])

        from sklearn.metrics import confusion_matrix

        self.confusion_matrix = confusion_matrix(all_y_true, all_y_pred)

    def get_metric_summary(self, metric_name: str) -> Dict[str, float]:
        """Get statistical summary for a specific metric."""
        return {
            "mean": self.mean_metrics.get(metric_name, 0.0),
            "std": self.std_metrics.get(metric_name, 0.0),
            "min": self.min_metrics.get(metric_name, 0.0),
            "max": self.max_metrics.get(metric_name, 0.0),
            "cv": self.std_metrics.get(metric_name, 0.0)
            / max(
                self.mean_metrics.get(metric_name, 1.0), 1e-10
            ),  # Coefficient of variation
        }

    def get_mean_metric(self, metric_name: str) -> float:
        """Get mean value for a specific metric."""
        return self.mean_metrics.get(metric_name, 0.0)

    def get_fold_stability(self, metric_name: str) -> str:
        """
        Assess stability of a metric across folds.

        Returns:
        --------
        str: "stable", "moderate", or "unstable"
        """
        if not self.fold_results or metric_name not in self.std_metrics:
            return "unknown"

        mean_val = self.mean_metrics.get(metric_name, 0.0)
        std_val = self.std_metrics.get(metric_name, 0.0)

        # Use coefficient of variation to assess stability
        cv = std_val / max(mean_val, 1e-10)

        if cv < 0.1:
            return "stable"
        elif cv < 0.2:
            return "moderate"
        else:
            return "unstable"

    def get_mlflow_metrics(self) -> Dict[str, float]:
        """Get aggregated metrics suitable for MLflow logging."""
        metrics = {}

        # Add mean metrics (primary metrics)
        for metric_name, value in self.mean_metrics.items():
            if isinstance(value, (int, float)) and not np.isnan(value):
                metrics[metric_name] = float(value)

        # Add statistical measures with suffix
        for metric_name, value in self.std_metrics.items():
            if isinstance(value, (int, float)) and not np.isnan(value):
                metrics[f"{metric_name}_std"] = float(value)

        for metric_name, value in self.min_metrics.items():
            if isinstance(value, (int, float)) and not np.isnan(value):
                metrics[f"{metric_name}_min"] = float(value)

        for metric_name, value in self.max_metrics.items():
            if isinstance(value, (int, float)) and not np.isnan(value):
                metrics[f"{metric_name}_max"] = float(value)

        # Add timing information if available
        if self.fold_results:
            train_times = [fold.training_time for fold in self.fold_results]
            pred_times = [fold.prediction_time for fold in self.fold_results]

            metrics.update(
                {
                    "avg_training_time": np.mean(train_times),
                    "avg_prediction_time": np.mean(pred_times),
                    "total_training_time": np.sum(train_times),
                    "total_prediction_time": np.sum(pred_times),
                }
            )

        return metrics

    def __repr__(self) -> str:
        f1 = self.get_mean_metric("f1_score")
        f1_std = self.std_metrics.get("f1_score", 0)
        return f"ModelResult({self.model_name}, f1={f1:.2f}Â±{f1_std:.2f}, {len(self.fold_results)} folds)"
